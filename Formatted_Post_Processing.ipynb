{"cells":[{"cell_type":"markdown","metadata":{"id":"2UrkzABENORx"},"source":["## Post processing\n","\n","The Notebook function performs data refinement tasks on JSON files(API result JSON file) derived from PDFs. It extracts essential fields specified under the 'content' key, utilizing bounding box coordinates obtained from the PDFs. The function then organizes this data into a structured format and stores it in an output JSON file, where each field is paired with its corresponding key."]},{"cell_type":"markdown","metadata":{"id":"0s-B_FkTNOR4"},"source":["#### Importing the necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_sfbRwiNOR5"},"outputs":[],"source":["import json\n","import pandas as pd\n","import os\n","import re\n","import time"]},{"cell_type":"markdown","metadata":{"id":"fdotLHPHNOR5"},"source":["#### Custom Error class\n","This ErrorClass raise an `ProcessingError`, if the logic fails to process a document\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bl6FLVPDNOR5"},"outputs":[],"source":["class ProcessingError(Exception):\n","    \"\"\"\n","    Exception raised for errors during document processing.\n","\n","    Attributes:\n","        message (str): Explanation of the error.\n","    \"\"\"\n","\n","    def __init__(self, message=\"Error during document processing\"):\n","        \"\"\"\n","        Initialize the ProcessingError.\n","\n","        Args:\n","            message (str, optional): Explanation of the error. Defaults to \"Error during document processing\".\n","        \"\"\"\n","        self.message = message\n","        super().__init__(self.message)"]},{"cell_type":"markdown","metadata":{"id":"92bV3nYKNOR5"},"source":["### Post Processing Layer\n","This layer processes the JSON files and extract all the neccessary fields from the input JSON and generates the output Json file which contains all the fields extracted by the post_processing_layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkEK_q34NOR5"},"outputs":[],"source":["def post_processing_layer(api_result_file_name, input_folder_path, output_folder_path):\n","    \"\"\"\n","    Perform post-processing on API result json data extracted from PDF files.\n","\n","    The algorithm retrieves the essential fields identified under the 'content'\n","    key within the input JSON file(API result JSON file), utilizing coordinates from bounding boxes\n","    extracted from the PDF files. Subsequently, it stores these fields into the\n","    output JSON file, where each field is presented in the format of key-value pairs.\n","\n","    Input:\n","        JSON file (API result JSON file)\n","    Args:\n","        api_result_file_name (list): List of filenames of the extracted data in JSON format.\n","        input_folder_path (str): Path to the directory containing the extracted data in JSON format.\n","        output_folder_path (str): Path to the output folder where the processed data will be stored.\n","\n","    Raises:\n","        ProcessingError: If logic fails to process a document.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    # Construct paths for input JSON files\n","    test_20_res = [input_folder_path + i for i in api_result_file_name]\n","\n","    # List to store failed documents\n","    failed_docs = []\n","\n","    # Iterate over each JSON file\n","    for json_path in test_20_res:\n","        try:\n","            # Loading the JSON file\n","            with open(json_path) as f:\n","                res_json = json.load(f)\n","\n","            # Initialize dictionary to store extracted data\n","            result_dict = {\n","                \"page_num\": [],\n","                \"content\": [],\n","                \"top_left_x\": [],\n","                \"top_left_y\": [],\n","                \"top_right_x\": [],\n","                \"bottom_left_y\": [],\n","            }\n","\n","            # Iterate over each page in the JSON data\n","            for pg_data in res_json[\"pages\"]:\n","                # Iterate over each line in the page\n","                for line in pg_data[\"lines\"]:\n","                    # Extract relevant information and append to result_dict\n","                    result_dict[\"page_num\"].append(pg_data[\"page_number\"])\n","                    result_dict[\"content\"].append(line[\"content\"])\n","                    result_dict[\"top_left_x\"].append(line[\"polygon\"][0][\"x\"])\n","                    result_dict[\"top_left_y\"].append(line[\"polygon\"][0][\"y\"])\n","                    result_dict[\"top_right_x\"].append(line[\"polygon\"][1][\"x\"])\n","                    result_dict[\"bottom_left_y\"].append(line[\"polygon\"][3][\"y\"])\n","\n","            # Convert result_dict to DataFrame\n","            df = pd.DataFrame(result_dict)\n","\n","            # Add calculated columns 'mid_x' and 'mid_y'\n","            df[\"mid_x\"] = (df[\"top_left_x\"] + df[\"top_right_x\"]) / 2\n","            df[\"mid_y\"] = (df[\"top_left_y\"] + df[\"bottom_left_y\"]) / 2\n","\n","            # Filter out rows with content 'RM'\n","            df2 = df[df[\"content\"] != \"RM\"]\n","\n","            # Capturing data based on rows from dataframe\n","            # Initialize nested_row_dict to store grouped content\n","            nested_row_dict = {}\n","\n","            # Group content based on proximity of mid_y\n","            # Iterate over each page in df2\n","            for page_num, page_df in df2.groupby(\"page_num\"):\n","                # Calculate threshold for grouping based on mean mid_y difference\n","                # Adjust this value as needed\n","                threshold = page_df[\"mid_y\"].diff().mean()\n","\n","                # Initialize variables to track grouped content\n","                grouped_content = []\n","                current_group = []\n","                prev_mid_y = None\n","\n","                # Iterate over each row in the page_df\n","                for index, row in page_df.iterrows():\n","                    # Check proximity of mid_y with previous row\n","                    if (\n","                        prev_mid_y is None\n","                        or abs(row[\"mid_y\"] - prev_mid_y) <= threshold\n","                    ):\n","                        current_group.append(row[\"content\"])\n","                    else:\n","                        # Append current group to grouped_content and start a new group\n","                        grouped_content.append(current_group)\n","                        current_group = [row[\"content\"]]\n","                    prev_mid_y = row[\"mid_y\"]\n","\n","                # Append the last group\n","                if current_group:\n","                    grouped_content.append(current_group)\n","\n","                # Construct page_dict from grouped_content\n","                page_dict = {group[0]: group[1:] for group in grouped_content}\n","\n","                # Add page_dict to nested_row_dict\n","                nested_row_dict[page_num] = page_dict\n","\n","            # Extracting column data yearwise\n","\n","            years = [\n","                \"2008\",\n","                \"2009\",\n","                \"2010\",\n","                \"2011\",\n","                \"2012\",\n","                \"2013\",\n","                \"2014\",\n","                \"2015\",\n","                \"2016\",\n","                \"2017\",\n","                \"2018\",\n","                \"2019\",\n","                \"2020\",\n","                \"2021\",\n","                \"2022\",\n","                \"2023\",\n","            ]\n","\n","            # Filter rows with content in years list\n","            temp = df2[df2[\"content\"].isin(years)]\n","            years_present = list(temp[\"content\"].unique())\n","            print(years_present)\n","\n","            # Initialize col_page_dict to store column data\n","            col_page_dict = {}\n","\n","            # Group df2 by page_num\n","            grouped_df2 = df2.groupby(\"page_num\")\n","\n","            # Iterate over each page_num group\n","            for page_num, group in grouped_df2:\n","                col_page_dict[page_num] = {}\n","\n","                # Calculate threshold only if the year is present in this page_num\n","                if all(year in group[\"content\"].values for year in years_present):\n","                    threshold_list = []\n","\n","                    # Calculate threshold for each year\n","                    if len(years_present) > 1:\n","                        for i in range(len(years_present) - 1):\n","                            difference = abs(\n","                                df2.loc[\n","                                    (df2[\"content\"] == years_present[i])\n","                                    & (df2[\"page_num\"] == page_num),\n","                                    \"mid_x\",\n","                                ].values[0]\n","                                - df2.loc[\n","                                    (df2[\"content\"] == years_present[i + 1])\n","                                    & (df2[\"page_num\"] == page_num),\n","                                    \"mid_x\",\n","                                ].values[0]\n","                            )\n","                            threshold_list.append(difference)\n","\n","                        if len(threshold_list) > 1:\n","                            threshold_list.append(threshold_list[-2])\n","                        else:\n","                            threshold_list.append(threshold_list[0])\n","                    else:\n","                        threshold_list = [\n","                            df2.loc[\n","                                (df2[\"content\"] == years_present[0])\n","                                & (df2[\"page_num\"] == page_num),\n","                                \"mid_x\",\n","                            ].values[0]\n","                        ]\n","\n","                    # Iterate over each year present in the page_num\n","                    for i, year in enumerate(years_present):\n","                        target_num = df2.loc[\n","                            (df2[\"content\"] == year) & (df2[\"page_num\"] == page_num),\n","                            \"mid_x\",\n","                        ].values\n","\n","                        if len(target_num) > 0:\n","                            target_num = target_num[0]\n","                            col_page_dict[page_num][year] = []\n","\n","                            # Adjust threshold calculation as needed for each page_num\n","                            threshold = threshold_list[i] - 0.5\n","\n","                            for index, row in group.iterrows():\n","                                if abs(row[\"mid_x\"] - target_num) < threshold:\n","                                    if row[\"content\"] != year:\n","                                        col_page_dict[page_num][year].append(\n","                                            row[\"content\"]\n","                                        )\n","\n","            # Final dictionary in required format\n","            table_data_dict = {}\n","            for page_num in col_page_dict.keys():\n","                table_data_dict[page_num] = {}\n","\n","                for year in years_present:\n","                    table_data_dict[page_num][year] = {}\n","                    if year in col_page_dict[page_num]:\n","                        for key, values in nested_row_dict[page_num].items():\n","                            # print(values)\n","                            for value in values:\n","                                if value in col_page_dict[page_num][year]:\n","                                    table_data_dict[page_num][year][key] = value\n","\n","                ## Adding the local number into teh post processed json file\n","                for line in result_dict[\"content\"]:\n","                    # Check if the line contains the string 'company number'\n","                    if \"Company No\" in line:\n","                        # If it does, Split the line by the string \"Company No.\"\n","                        parts = line.split(\"Company No\")\n","                        # The desired string will be the second part after the split\n","                        if len(parts) > 1:\n","                            # Remove any leading or trailing whitespace\n","                            company_number = parts[1].strip()\n","                            company_number1 = re.sub(\n","                                r\"[^\\w\\s-]\", \"\", str(company_number)\n","                            )\n","                        else:\n","                            print(\"Company number not found.\")\n","                        break\n","                # Add the key-value pair to the table_data_dict dictionary\n","                for inner_dict in table_data_dict.values():\n","                    if inner_dict:\n","                        # Add the key-value pair to the first nested dictionary\n","                        inner_dict[\"Local No\"] = company_number1\n","                        break\n","\n","            # Construct destination path for the processed JSON file\n","            dest_path = os.path.join(output_folder_path, os.path.basename(json_path))\n","\n","            # Write table_data_dict to the destination path\n","            with open(dest_path, \"w\") as json_file:\n","                json.dump(table_data_dict, json_file, indent=2)\n","            print()\n","            print(\n","                \"----------------\" + str(test_20_res.index(json_path)) + \"-------------\"\n","            )\n","            print()\n","        except:\n","            # Handle errors and append failed documents to failed_docs list\n","            failed_docs.append(json_path)\n","            print(\n","                f\"-----------------Logic falied for {json_path}-----------------------\"\n","            )\n","\n","        # Raise an error if any documents failed during processing\n","        if failed_docs:\n","            raise ProcessingError(\n","                f\"Failed to process the following documents: {failed_docs}\"\n","            )\n","\n","        return"]},{"cell_type":"markdown","metadata":{"id":"4vR1UXfMNOR6"},"source":["### Description:\n","\n","- This code snippet performs post-processing on data extracted from a PDF file.\n","- Defines input and output paths\n","- Records start and end times\n","- Executes the post-processing function\n","- Calculates the time taken, and prints the result.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCdIPU9RNOR6","outputId":"8f35699c-b5f7-4648-ed8b-5588aec21089"},"outputs":[{"name":"stdout","output_type":"stream","text":["['2017', '2016']\n","\n","----------------0-------------\n","\n","Time taken for the Post Processing: 0.39785170555114746 seconds\n"]}],"source":["# Define paths for input and output folders\n","api_result_file_name = [\"API_result_file_name.json\"]\n","input_folder_path = \"path/to/input/folder\"\n","output_folder_path = \"path/to/output/folder\"\n","\n","# Record start time\n","start_time = time.time()\n","\n","# Perform post-processing\n","post_processing_layer(api_result_file_name, input_folder_path, output_folder_path)\n","\n","# Record end time\n","end_time = time.time()\n","\n","# Calculate the time taken\n","time_taken = end_time - start_time\n","\n","# Print the time taken\n","print(f\"Time taken for the Post Processing: {time_taken} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ozsDGQwNOR6"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.9 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"2607249a32e8f6de13c84eb64718733d6a4be855cb7796a207f18262c3e9c889"}}},"nbformat":4,"nbformat_minor":0}
